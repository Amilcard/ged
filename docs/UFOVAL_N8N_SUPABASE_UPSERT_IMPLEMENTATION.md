# UFOVAL n8n ‚Üí Supabase : Impl√©mentation sans r√©gression

**Workflow** : `GED__UFOVAL__SCRAPE_SEED_STAYS__v1`
**Date** : 30 janvier 2026
**Mode** : Economy Secure No Regression

---

## üéØ Objectif

Ajouter l'√©criture automatique dans Supabase (`gd_stays` + `gd_stay_sessions`) **SANS modifier** le comportement actuel d'export JSON du workflow n8n.

### Contraintes strictes
- ‚úÖ JSON export reste identique (source de v√©rit√©)
- ‚úÖ Upserts idempotents (re-run safe)
- ‚úÖ Rollback instantan√© = d√©sactiver 4 nouveaux n≈ìuds
- ‚úÖ Pas de breaking changes sur scraping/reformulation

---

## üìã √âTAPE 0 : Pr√©flight - V√©rifications pr√©alables

### 0.1 V√©rifier le chemin absolu du fichier JSON

**Via SSH sur Hostinger :**

```bash
# Se connecter au container n8n (adapter selon votre setup)
docker exec -it n8n /bin/sh
# OU si installation directe :
cd ~

# Trouver le r√©pertoire de travail n8n
echo $N8N_USER_FOLDER
# Ou chercher manuellement
find / -name "ufoval_seed_*.json" 2>/dev/null | head -5

# V√©rifier le fichier du jour
ls -lh /home/node/.n8n/docs/automations/GED/exports/ufoval_seed_2026-01-30.json
# OU le chemin que vous trouvez

# Compter les items
cat <CHEMIN_FICHIER> | jq '. | length'
# Devrait retourner ~30

# V√©rifier la structure d'un item (le premier)
cat <CHEMIN_FICHIER> | jq '.[0] | keys'
# Doit contenir : source_url, sessions_json, pro, kids, slug...
```

**‚úÖ Acceptation** :
- Chemin absolu confirm√©
- Fichier existe avec ~30 items
- Champs obligatoires pr√©sents : `source_url`, `sessions_json`, `pro.title_pro`, `kids.title_kids`

---

## üìã √âTAPE 1 : Supabase - Index UNIQUE et guardrails

### 1.1 Cr√©er les index de d√©duplication

Ouvrir **Supabase SQL Editor** et ex√©cuter :

```sql
-- Index UNIQUE sur source_url pour √©viter doublons de stays
CREATE UNIQUE INDEX IF NOT EXISTS uniq_gd_stays_source_url
ON public.gd_stays(source_url)
WHERE source_url IS NOT NULL;

-- Index UNIQUE composite pour √©viter doublons de sessions
-- (m√™me s√©jour + m√™mes dates = 1 seule ligne)
CREATE UNIQUE INDEX IF NOT EXISTS uniq_gd_stay_sessions_slug_dates
ON public.gd_stay_sessions(stay_slug, start_date, end_date);

-- V√©rifier la cr√©ation
SELECT
  schemaname,
  tablename,
  indexname
FROM pg_indexes
WHERE tablename IN ('gd_stays', 'gd_stay_sessions')
  AND indexname LIKE 'uniq_%';
```

**‚úÖ Attendu** : 2 lignes retourn√©es avec les noms d'index `uniq_gd_stays_source_url` et `uniq_gd_stay_sessions_slug_dates`.

### 1.2 V√©rifier RLS et service role

```sql
-- V√©rifier si RLS est actif
SELECT tablename, rowsecurity
FROM pg_tables
WHERE schemaname = 'public'
  AND tablename IN ('gd_stays', 'gd_stay_sessions');

-- Si rowsecurity = true, v√©rifier que le service role bypass fonctionne
-- (le service role key bypass automatiquement RLS, mais v√©rifier qu'il n'y a pas de policies bloquantes)
```

**Note** : Le service role key configur√© dans n8n **bypass automatiquement RLS**. Pas d'action suppl√©mentaire n√©cessaire.

---

## üìã √âTAPE 2 : n8n - Ajouter le n≈ìud FILTER (optionnel mais recommand√©)

### 2.1 Position dans le workflow

```
[Aggregate Final]
    ‚Üì
[Format Export]
    ‚Üì
[Write JSON Export]  ‚Üê NE PAS MODIFIER
    ‚Üì
[FILTER__VALID_ITEMS_FOR_DB]  ‚Üê NOUVEAU ‚ú®
    ‚Üì (branche TRUE)
[HTTP__UPSERT_GD_STAYS]
```

### 2.2 Configuration du n≈ìud FILTER

**Type de n≈ìud** : `IF` (ou `Filter` selon version n8n)
**Nom** : `FILTER__VALID_ITEMS_FOR_DB`

**Conditions (mode AND - toutes doivent √™tre vraies) :**

| Champ | Op√©ration | Valeur |
|-------|-----------|--------|
| `{{ $json.source_url }}` | is not empty | - |
| `{{ $json.pro.title_pro }}` | is not empty | - |
| `{{ $json.kids.title_kids }}` | is not empty | - |
| `{{ $json.sessions_json }}` | is not empty | - |

**Expression alternative (si mode expression disponible) :**

```javascript
{{
  $json.source_url &&
  $json.pro?.title_pro &&
  $json.kids?.title_kids &&
  ($json.sessions_json && (
    (Array.isArray($json.sessions_json) && $json.sessions_json.length > 0) ||
    (typeof $json.sessions_json === 'string' && $json.sessions_json.length > 2)
  ))
}}
```

**Routing** :
- **TRUE** ‚Üí Continue vers HTTP__UPSERT_GD_STAYS
- **FALSE** ‚Üí (optionnel) connecter √† un n≈ìud `No Operation` ou laisser terminer

---

## üìã √âTAPE 3 : n8n - Upsert gd_stays

### 3.1 Configuration du n≈ìud HTTP Request

**Type** : `HTTP Request`
**Nom** : `HTTP__UPSERT_GD_STAYS`
**Connect√© depuis** : `FILTER__VALID_ITEMS_FOR_DB` (sortie TRUE)

#### Param√®tres de base

| Param√®tre | Valeur |
|-----------|--------|
| **Method** | `POST` |
| **URL** | `={{ $credentials.supabase.host }}/rest/v1/gd_stays` |

**Note** : Si `$credentials.supabase.host` ne fonctionne pas, utiliser : `https://VOTRE_PROJECT_REF.supabase.co/rest/v1/gd_stays`

#### Query Parameters

| Nom | Valeur |
|-----|--------|
| `on_conflict` | `source_url` |

#### Headers

| Nom | Valeur |
|-----|--------|
| `apikey` | `={{ $credentials.supabase.serviceRoleKey }}` |
| `Authorization` | `Bearer {{ $credentials.supabase.serviceRoleKey }}` |
| `Content-Type` | `application/json` |
| `Prefer` | `resolution=merge-duplicates,return=representation` |

**Note** : Adapter `$credentials.supabase.serviceRoleKey` au nom exact de votre credential n8n.

#### Body (Send Body : Yes, Body Content Type : JSON)

```javascript
={{
  $input.all().map(item => ({
    source_url: item.json.source_url,
    slug: item.json.slug || item.json.source_url.split('/').pop().replace(/[^a-z0-9-]/gi, '-').toLowerCase(),
    title_pro: item.json.pro.title_pro,
    title_kids: item.json.kids.title_kids,
    description_pro: item.json.pro?.description_pro || null,
    description_kids: item.json.kids?.description_kids || null,
    sessions_json: typeof item.json.sessions_json === 'string'
      ? item.json.sessions_json
      : JSON.stringify(item.json.sessions_json),
    import_batch_ts: new Date().toISOString()
  }))
}}
```

**Options avanc√©es** :
- **Split Into Items** : `No` (on envoie un array)
- **Ignore SSL Issues** : `No`
- **Response Format** : `JSON`

---

## üìã √âTAPE 4 : n8n - Transform sessions en lignes

### 4.1 Configuration du n≈ìud Function

**Type** : `Function` ou `Code`
**Nom** : `TRANSFORM__SESSIONS_TO_ROWS`
**Connect√© depuis** : `HTTP__UPSERT_GD_STAYS`

#### Code JavaScript

```javascript
const output = [];

for (const item of $input.all()) {
  const stay = item.json;

  // G√©rer sessions_json qui peut √™tre string ou array
  let sessions = [];
  try {
    if (Array.isArray(stay.sessions_json)) {
      sessions = stay.sessions_json;
    } else if (typeof stay.sessions_json === 'string') {
      sessions = JSON.parse(stay.sessions_json);
    } else if (stay.sessions_json && typeof stay.sessions_json === 'object') {
      sessions = [stay.sessions_json]; // Single session as object
    }
  } catch (e) {
    console.error(`Failed to parse sessions_json for stay ${stay.slug}:`, e);
    continue;
  }

  // Cr√©er une ligne par session
  for (const session of sessions) {
    output.push({
      json: {
        stay_slug: stay.slug || stay.source_url?.split('/').pop().replace(/[^a-z0-9-]/gi, '-').toLowerCase(),
        start_date: session.start_date || session.date_debut || session.dateDebut || null,
        end_date: session.end_date || session.date_fin || session.dateFin || null,
        seats_left: session.seats_left ?? session.places_restantes ?? session.placesRestantes ?? null,
        city_departure: session.city_departure ?? session.ville_depart ?? session.villeDepart ?? null,
        price: session.price ?? session.tarif ?? session.prix ?? null,
        age_min: session.age_min ?? session.ageMin ?? null,
        age_max: session.age_max ?? session.ageMax ?? null,
        import_batch_ts: new Date().toISOString()
      }
    });
  }
}

return output;
```

**‚úÖ Acceptation** : Le n≈ìud doit produire N items o√π N = somme de toutes les sessions de tous les stays.

---

## üìã √âTAPE 5 : n8n - Upsert gd_stay_sessions

### 5.1 Configuration du n≈ìud HTTP Request

**Type** : `HTTP Request`
**Nom** : `HTTP__UPSERT_GD_STAY_SESSIONS`
**Connect√© depuis** : `TRANSFORM__SESSIONS_TO_ROWS`

#### Param√®tres de base

| Param√®tre | Valeur |
|-----------|--------|
| **Method** | `POST` |
| **URL** | `={{ $credentials.supabase.host }}/rest/v1/gd_stay_sessions` |

#### Query Parameters

| Nom | Valeur |
|-----|--------|
| `on_conflict` | `stay_slug,start_date,end_date` |

#### Headers

| Nom | Valeur |
|-----|--------|
| `apikey` | `={{ $credentials.supabase.serviceRoleKey }}` |
| `Authorization` | `Bearer {{ $credentials.supabase.serviceRoleKey }}` |
| `Content-Type` | `application/json` |
| `Prefer` | `resolution=merge-duplicates,return=representation` |

#### Body (Send Body : Yes, Body Content Type : JSON)

```javascript
={{
  $input.all().map(item => ({
    stay_slug: item.json.stay_slug,
    start_date: item.json.start_date,
    end_date: item.json.end_date,
    seats_left: item.json.seats_left,
    city_departure: item.json.city_departure,
    price: item.json.price,
    age_min: item.json.age_min,
    age_max: item.json.age_max,
    import_batch_ts: item.json.import_batch_ts
  }))
}}
```

---

## üìã √âTAPE 6 : Validation Supabase

### 6.1 Requ√™tes SQL de v√©rification

Ex√©cuter dans **Supabase SQL Editor** apr√®s un run du workflow :

```sql
-- 1. Compter les stays import√©s dans les derni√®res 24h
SELECT
  count(*) as total_stays,
  max(import_batch_ts) as last_import
FROM public.gd_stays
WHERE import_batch_ts >= (now() - interval '24 hours');
-- Attendu : ~30 stays

-- 2. Compter les sessions import√©es dans les derni√®res 24h
SELECT
  count(*) as total_sessions,
  max(import_batch_ts) as last_import
FROM public.gd_stay_sessions
WHERE import_batch_ts >= (now() - interval '24 hours');
-- Attendu : total coh√©rent avec la somme des sessions dans le JSON

-- 3. V√©rifier l'absence de doublons de stays
SELECT
  source_url,
  count(*) as duplicates
FROM public.gd_stays
GROUP BY source_url
HAVING count(*) > 1;
-- Attendu : 0 lignes

-- 4. V√©rifier l'absence de doublons de sessions
SELECT
  stay_slug,
  start_date,
  end_date,
  count(*) as duplicates
FROM public.gd_stay_sessions
GROUP BY stay_slug, start_date, end_date
HAVING count(*) > 1;
-- Attendu : 0 lignes

-- 5. V√©rifier la coh√©rence stays ‚Üî sessions
SELECT
  s.slug,
  s.title_pro,
  count(ss.id) as session_count
FROM public.gd_stays s
LEFT JOIN public.gd_stay_sessions ss ON ss.stay_slug = s.slug
WHERE s.import_batch_ts >= (now() - interval '24 hours')
GROUP BY s.slug, s.title_pro
ORDER BY session_count DESC
LIMIT 10;
-- V√©rifier que chaque stay a au moins 1 session

-- 6. D√©tecter les sessions orphelines (sans stay parent)
SELECT
  ss.stay_slug,
  count(*) as orphan_count
FROM public.gd_stay_sessions ss
LEFT JOIN public.gd_stays s ON s.slug = ss.stay_slug
WHERE s.id IS NULL
  AND ss.import_batch_ts >= (now() - interval '24 hours')
GROUP BY ss.stay_slug;
-- Attendu : 0 lignes
```

### 6.2 Crit√®res de validation

| Test | R√©sultat attendu |
|------|------------------|
| Total stays import√©s | ~30 (ou nombre d'items valides apr√®s FILTER) |
| Total sessions import√©es | Somme coh√©rente avec sessions_json |
| Doublons stays | 0 |
| Doublons sessions | 0 |
| Sessions orphelines | 0 |
| Stays sans sessions | 0 (tous les stays doivent avoir ‚â•1 session) |

---

## üìã √âTAPE 7 : Rollback et d√©sactivation rapide

### 7.1 Proc√©dure de rollback (si probl√®me en prod)

**Dans l'√©diteur n8n :**

1. Ouvrir le workflow `GED__UFOVAL__SCRAPE_SEED_STAYS__v1`
2. **D√©sactiver** (bouton toggle) les 4 nouveaux n≈ìuds :
   - ‚ùå `FILTER__VALID_ITEMS_FOR_DB`
   - ‚ùå `HTTP__UPSERT_GD_STAYS`
   - ‚ùå `TRANSFORM__SESSIONS_TO_ROWS`
   - ‚ùå `HTTP__UPSERT_GD_STAY_SESSIONS`
3. Sauvegarder le workflow
4. **V√©rifier** : le prochain run doit produire le JSON export normalement, sans √©criture DB

**‚úÖ R√©sultat** : Retour au comportement exact d'avant (JSON export seul).

### 7.2 Grouping recommand√© dans n8n

Pour faciliter l'identification et la d√©sactivation :

1. S√©lectionner les 4 nouveaux n≈ìuds
2. Clic droit ‚Üí `Add to group` ou utiliser la fonction de grouping
3. Nommer le groupe : `üîÑ DB UPSERT (d√©sactivable)`
4. Ajouter une note sticky :

```
‚ö†Ô∏è ROLLBACK RAPIDE
Pour d√©sactiver l'√©criture DB sans toucher au scraping :
1. D√©sactiver tous les n≈ìuds de ce groupe
2. Le workflow continuera √† produire le JSON export normalement
```

---

## üìã √âTAPE 8 : Tests end-to-end

### 8.1 Sc√©nario de test complet

**Test 1 : Premier import (fresh data)**

1. Vider les tables (environnement de test uniquement) :
   ```sql
   TRUNCATE public.gd_stay_sessions, public.gd_stays CASCADE;
   ```

2. Ex√©cuter le workflow n8n manuellement

3. V√©rifier :
   - ‚úÖ Fichier JSON cr√©√© avec timestamp du jour
   - ‚úÖ ~30 stays dans `gd_stays`
   - ‚úÖ N sessions dans `gd_stay_sessions` (N = total des sessions)
   - ‚úÖ Aucun doublon

**Test 2 : Re-run (idempotence)**

1. **Sans modifier les donn√©es**, r√©-ex√©cuter le workflow imm√©diatement

2. V√©rifier :
   - ‚úÖ Fichier JSON mis √† jour (nouveau timestamp)
   - ‚úÖ Nombre de stays identique (upsert = merge, pas de nouveau insert)
   - ‚úÖ Nombre de sessions identique
   - ‚úÖ `import_batch_ts` mis √† jour sur les lignes existantes

3. Requ√™te de validation :
   ```sql
   SELECT
     'stays' as table_name,
     count(*) as total,
     count(DISTINCT import_batch_ts) as distinct_batch_ts
   FROM public.gd_stays
   UNION ALL
   SELECT
     'sessions',
     count(*),
     count(DISTINCT import_batch_ts)
   FROM public.gd_stay_sessions;
   ```
   **Attendu** : `distinct_batch_ts` = 1 (tous mis √† jour au m√™me moment lors du dernier run)

**Test 3 : Donn√©es incompl√®tes (filtre)**

1. Modifier manuellement 1 item dans le JSON export pour retirer `source_url`

2. R√©-ex√©cuter le workflow

3. V√©rifier :
   - ‚úÖ L'item incomplet est filtr√© (n'appara√Æt pas en DB)
   - ‚úÖ Les 29 autres items sont bien ins√©r√©s/mis √† jour
   - ‚úÖ Aucune erreur dans les logs n8n

---

## üéØ Definition of Done

### Checklist finale

- [ ] Le workflow produit `ufoval_seed_<date>.json` dans le dossier confirm√© (chemin absolu v√©rifi√©)
- [ ] Le m√™me run upsert ~30 stays dans `public.gd_stays`
- [ ] Le m√™me run upsert toutes les sessions dans `public.gd_stay_sessions`
- [ ] Aucun doublon apr√®s 2 runs cons√©cutifs (requ√™tes SQL de validation OK)
- [ ] `import_batch_ts` est identique pour tous les items d'un m√™me run
- [ ] Les index UNIQUE sont actifs et emp√™chent les doublons manuels
- [ ] Si les 4 nouveaux n≈ìuds sont d√©sactiv√©s, le workflow exporte le JSON exactement comme avant
- [ ] Documentation de rollback cr√©√©e et test√©e
- [ ] Logs n8n ne montrent aucune erreur 5xx ou timeout Supabase

---

## üìö Annexes

### A. Sch√©ma des tables Supabase (r√©f√©rence)

```sql
-- Table gd_stays (simplifi√©)
CREATE TABLE IF NOT EXISTS public.gd_stays (
  id BIGSERIAL PRIMARY KEY,
  source_url TEXT UNIQUE NOT NULL,
  slug TEXT NOT NULL,
  title_pro TEXT NOT NULL,
  title_kids TEXT NOT NULL,
  description_pro TEXT,
  description_kids TEXT,
  sessions_json JSONB,
  import_batch_ts TIMESTAMPTZ DEFAULT now(),
  created_at TIMESTAMPTZ DEFAULT now(),
  updated_at TIMESTAMPTZ DEFAULT now()
);

-- Table gd_stay_sessions (simplifi√©)
CREATE TABLE IF NOT EXISTS public.gd_stay_sessions (
  id BIGSERIAL PRIMARY KEY,
  stay_slug TEXT NOT NULL REFERENCES public.gd_stays(slug) ON DELETE CASCADE,
  start_date DATE NOT NULL,
  end_date DATE NOT NULL,
  seats_left INTEGER,
  city_departure TEXT,
  price NUMERIC(10,2),
  age_min INTEGER,
  age_max INTEGER,
  import_batch_ts TIMESTAMPTZ DEFAULT now(),
  created_at TIMESTAMPTZ DEFAULT now(),
  UNIQUE(stay_slug, start_date, end_date)
);

-- Indexes (d√©j√† cr√©√©s √† l'√©tape 1)
CREATE UNIQUE INDEX IF NOT EXISTS uniq_gd_stays_source_url
ON public.gd_stays(source_url) WHERE source_url IS NOT NULL;

CREATE UNIQUE INDEX IF NOT EXISTS uniq_gd_stay_sessions_slug_dates
ON public.gd_stay_sessions(stay_slug, start_date, end_date);
```

### B. Mapping des champs JSON ‚Üí DB

| JSON (scraped) | DB Column (stays) | Notes |
|----------------|-------------------|-------|
| `source_url` | `source_url` | UNIQUE, obligatoire |
| `slug` | `slug` | G√©n√©r√© si manquant |
| `pro.title_pro` | `title_pro` | Obligatoire |
| `kids.title_kids` | `title_kids` | Obligatoire |
| `pro.description_pro` | `description_pro` | Optionnel |
| `kids.description_kids` | `description_kids` | Optionnel |
| `sessions_json` | `sessions_json` | Stock√© en JSONB |

| JSON (session) | DB Column (sessions) | Notes |
|----------------|----------------------|-------|
| `start_date` / `date_debut` | `start_date` | Partie de UNIQUE composite |
| `end_date` / `date_fin` | `end_date` | Partie de UNIQUE composite |
| `seats_left` / `places_restantes` | `seats_left` | Optionnel |
| `city_departure` / `ville_depart` | `city_departure` | Optionnel |
| `price` / `tarif` / `prix` | `price` | Optionnel |
| `age_min` | `age_min` | Optionnel |
| `age_max` | `age_max` | Optionnel |

### C. Variables n8n √† adapter

Selon votre configuration de credentials n8n pour Supabase :

| Placeholder dans le doc | √Ä remplacer par |
|-------------------------|-----------------|
| `{{ $credentials.supabase.host }}` | L'URL de votre projet Supabase OU le nom de votre credential |
| `{{ $credentials.supabase.serviceRoleKey }}` | Le nom exact du champ contenant la service role key |

**Exemple si credential nomm√© "Supabase Flooow" :**
```javascript
URL: {{ $credentials['Supabase Flooow'].url }}/rest/v1/gd_stays
Header: {{ $credentials['Supabase Flooow'].serviceKey }}
```

---

## ‚úÖ Prochaines √©tapes

1. **V√©rifier le chemin absolu** du JSON export (√âTAPE 0)
2. **Cr√©er les index UNIQUE** sur Supabase (√âTAPE 1)
3. **Ajouter les 4 nouveaux n≈ìuds** dans n8n (√âTAPES 2-5)
4. **Tester** avec un run manuel (√âTAPE 8)
5. **Valider** avec les requ√™tes SQL (√âTAPE 6)
6. **Documenter** le rollback dans le workflow (√âTAPE 7)

---

**Document g√©n√©r√© le** : 30 janvier 2026
**Pour** : Projet Flooow ‚Äì GED UFOVAL Scraper
**Mode** : No Regression ‚Äì Safe to Deploy
